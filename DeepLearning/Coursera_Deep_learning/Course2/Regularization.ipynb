{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regularization.ipynb","provenance":[],"authorship_tag":"ABX9TyOljlR0OipBuZs/CYgA5oyB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"NyDO49Tm2I_C","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596047598419,"user_tz":-360,"elapsed":1036,"user":{"displayName":"Jobayed Ullah Shuvo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEb5dzZzsBsbJE-UM9AEEu3_vdtvDxQn1ABVaMnw=s64","userId":"01262328741808285025"}}},"source":["# import packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n","#from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n","import sklearn\n","import sklearn.datasets\n","import scipy.io\n","#from testCases import *\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfAfFpQE2Q0m","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596047975578,"user_tz":-360,"elapsed":742,"user":{"displayName":"Jobayed Ullah Shuvo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEb5dzZzsBsbJE-UM9AEEu3_vdtvDxQn1ABVaMnw=s64","userId":"01262328741808285025"}}},"source":["def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n","    \"\"\"\n","    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n","    learning_rate -- learning rate of the optimization\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- If True, print the cost every 10000 iterations\n","    lambd -- regularization hyperparameter, scalar\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n","    \n","    Returns:\n","    parameters -- parameters learned by the model. They can then be used to predict.\n","    \"\"\"\n","        \n","    grads = {}\n","    costs = []                            # to keep track of the cost\n","    m = X.shape[1]                        # number of examples\n","    layers_dims = [X.shape[0], 20, 3, 1]\n","    \n","    # Initialize parameters dictionary.\n","    parameters = initialize_parameters(layers_dims)\n","\n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n","        if keep_prob == 1:\n","            a3, cache = forward_propagation(X, parameters)\n","        elif keep_prob < 1:\n","            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n","        \n","        # Cost function\n","        if lambd == 0:\n","            cost = compute_cost(a3, Y)\n","        else:\n","            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n","            \n","        # Backward propagation.\n","        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n","                                            # but this assignment will only explore one at a time\n","        if lambd == 0 and keep_prob == 1:\n","            grads = backward_propagation(X, Y, cache)\n","        elif lambd != 0:\n","            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n","        elif keep_prob < 1:\n","            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n","        \n","        # Update parameters.\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        # Print the loss every 10000 iterations\n","        if print_cost and i % 10000 == 0:\n","            print(\"Cost after iteration {}: {}\".format(i, cost))\n","        if print_cost and i % 1000 == 0:\n","            costs.append(cost)\n","    \n","    # plot the cost\n","    plt.plot(costs)\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (x1,000)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"nidaxG2R32EE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596048750119,"user_tz":-360,"elapsed":894,"user":{"displayName":"Jobayed Ullah Shuvo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEb5dzZzsBsbJE-UM9AEEu3_vdtvDxQn1ABVaMnw=s64","userId":"01262328741808285025"}}},"source":["# GRADED FUNCTION: compute_cost_with_regularization\n","\n","def compute_cost_with_regularization(A3, Y, parameters, lambd):\n","    \"\"\"\n","    Implement the cost function with L2 regularization. See formula (2) above.\n","    \n","    Arguments:\n","    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    parameters -- python dictionary containing parameters of the model\n","    \n","    Returns:\n","    cost - value of the regularized loss function (formula (2))\n","    \"\"\"\n","    m = Y.shape[1]\n","    W1 = parameters[\"W1\"]\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","    \n","    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n","    \n","    ### START CODE HERE ### (approx. 1 line)\n","    L2_regularization_cost = lambd/(2*m)*(  np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3))  )\n","    ### END CODER HERE ###\n","    \n","    cost = cross_entropy_cost + L2_regularization_cost\n","    \n","    return cost\n","\n","\n","\n","\n","# GRADED FUNCTION: backward_propagation_with_regularization\n","\n","def backward_propagation_with_regularization(X, Y, cache, lambd):\n","    \"\"\"\n","    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    cache -- cache output from forward_propagation()\n","    lambd -- regularization hyperparameter, scalar\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    \n","    ### START CODE HERE ### (approx. 1 line)\n","    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m*W3\n","    ### END CODE HERE ###\n","    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n","    \n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    ### START CODE HERE ### (approx. 1 line)\n","    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m*W2\n","    ### END CODE HERE ###\n","    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    ### START CODE HERE ### (approx. 1 line)\n","    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m*W1\n","    ### END CODE HERE ###\n","    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n","                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmByRoW36Be8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596049791892,"user_tz":-360,"elapsed":621,"user":{"displayName":"Jobayed Ullah Shuvo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEb5dzZzsBsbJE-UM9AEEu3_vdtvDxQn1ABVaMnw=s64","userId":"01262328741808285025"}}},"source":["## DROPOUT\n","\n","def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n","    \"\"\"\n","    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (2, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape (20, 2)\n","                    b1 -- bias vector of shape (20, 1)\n","                    W2 -- weight matrix of shape (3, 20)\n","                    b2 -- bias vector of shape (3, 1)\n","                    W3 -- weight matrix of shape (1, 3)\n","                    b3 -- bias vector of shape (1, 1)\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar\n","    \n","    Returns:\n","    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n","    cache -- tuple, information stored for computing the backward propagation\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = relu(Z1)\n","    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n","    D1 = np.random.rand(A1.shape[0],A1.shape[1])      # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n","    D1 = (D1<keep_prob).astype(int)                   # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n","    A1 = np.multiply(A1,D1)                           # Step 3: shut down some neurons of A1\n","    A1 = A1/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n","    ### END CODE HERE ###\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = relu(Z2)\n","    ### START CODE HERE ### (approx. 4 lines)\n","    D2 = np.random.rand(A2.shape[0],A2.shape[1])      # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n","    D2 = (D2<keep_prob).astype(int)                   # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n","    A2 = np.multiply(A2,D2)                           # Step 3: shut down some neurons of A2\n","    A2 = A2/keep_prob                                 # Step 4: scale the value of neurons that haven't been shut down\n","    ### END CODE HERE ###\n","    Z3 = np.dot(W3, A2) + b3\n","    A3 = sigmoid(Z3)\n","    \n","    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n","    \n","    return A3, cache"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzfiRrlU-xhQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1596051446580,"user_tz":-360,"elapsed":795,"user":{"displayName":"Jobayed Ullah Shuvo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEb5dzZzsBsbJE-UM9AEEu3_vdtvDxQn1ABVaMnw=s64","userId":"01262328741808285025"}},"outputId":"d70c50ed-9e91-49f1-a9d5-d361bd2044df"},"source":["prob = np.random.rand(3,2)\n","\n","prob,(prob>.7).astype(int)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[0.48253382, 0.18306384],\n","        [0.55436213, 0.89740837],\n","        [0.04777411, 0.60250643]]), array([[0, 0],\n","        [0, 1],\n","        [0, 0]]))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"sGtkq0qnE4mD","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}